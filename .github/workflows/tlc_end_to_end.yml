name: NYC TLC End-to-End (Parquet â†’ CSV â†’ SQL)

on:
  workflow_dispatch:
    inputs:
      start_year:
        description: "Start year (e.g. 2022)"
        required: true
        default: "2022"

      start_month:
        description: "Start month (1â€“12)"
        required: true
        default: "10"

      end_year:
        description: "End year (e.g. 2025)"
        required: true
        default: "2025"

      end_month:
        description: "End month (1â€“12)"
        required: true
        default: "10"

      force_reprocess:
        description: "FORCE full reprocessing (ignore logs, rebuild CSVs)? yes/no"
        required: true
        default: "no"

      delete_existing_csv:
        description: "Delete existing CSV blobs before rebuild? yes/no"
        required: true
        default: "no"

  schedule:
    # Monthly auto-run: 02:15 UTC on day 2 of month
    - cron: "15 2 2 * *"

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # ---------------------------------------------
      # Required for pyodbc â†’ Azure SQL
      # ---------------------------------------------
      - name: Install Microsoft ODBC Driver 18
        run: |
          sudo apt-get update
          sudo apt-get install -y curl apt-transport-https gnupg
          curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
          curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list
          sudo apt-get update
          sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18 unixodbc-dev

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Debug â€“ list scripts directory
        run: |
          ls -la scripts

      # =========================================================
      # PHASE 1 â€” Download Parquet â†’ Azure Blob
      # =========================================================
      - name: Phase 1 â€“ Download Parquet and Upload to Blob
        env:
          STORAGE_ACCOUNT_URL: ${{ secrets.STORAGE_ACCOUNT_URL }}
          STORAGE_CONTAINER: ${{ secrets.STORAGE_CONTAINER }}
          STORAGE_ACCOUNT_KEY: ${{ secrets.STORAGE_ACCOUNT_KEY }}

          START_YEAR: ${{ github.event.inputs.start_year }}
          START_MONTH: ${{ github.event.inputs.start_month }}
          END_YEAR: ${{ github.event.inputs.end_year }}
          END_MONTH: ${{ github.event.inputs.end_month }}

          # Never auto-delete Parquet unless you explicitly change script
          DELETE_EXISTING: "no"
        run: |
          python scripts/download_yellow_parquet.py

      # =========================================================
      # PHASE 2 â€” Parquet â†’ CSV â†’ BULK INSERT â†’ FACT
      # =========================================================
      - name: Phase 2 â€“ Convert Parquet â†’ CSV â†’ Load Azure SQL
        env:
          STORAGE_ACCOUNT_URL: ${{ secrets.STORAGE_ACCOUNT_URL }}
          STORAGE_CONTAINER: ${{ secrets.STORAGE_CONTAINER }}
          STORAGE_ACCOUNT_KEY: ${{ secrets.STORAGE_ACCOUNT_KEY }}

          SQL_SERVER: ${{ secrets.SQL_SERVER }}
          SQL_DATABASE: ${{ secrets.SQL_DATABASE }}
          SQL_USERNAME: ${{ secrets.SQL_USERNAME }}
          SQL_PASSWORD: ${{ secrets.SQL_PASSWORD }}

          START_YEAR: ${{ github.event.inputs.start_year }}
          START_MONTH: ${{ github.event.inputs.start_month }}
          END_YEAR: ${{ github.event.inputs.end_year }}
          END_MONTH: ${{ github.event.inputs.end_month }}

          # ðŸ”¥ CONTROL FLAGS ðŸ”¥
          FORCE_REPROCESS: ${{ github.event.inputs.force_reprocess }}
          DELETE_EXISTING_CSV: ${{ github.event.inputs.delete_existing_csv }}

        run: |
          python scripts/parquet_to_csv_and_load_sql.py
